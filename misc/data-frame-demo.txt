spark-2.0.1-bin-hadoop2.7

scala-sbt.org/download.html

val a = sc . parallelize ( List (" dog " , " tiger " , " lion " , " cat " , " panther " , "eagle ") , 2)

/home/hduhttp://wiki.apache.org/hadoop/ConnectionRefused

texFile.foreach(println)

hdfs://localhost:9000/user/hduser/input

spark-shell --master local[1]

http://spark.apache.org/

./bin/run-example SparkPi

hadoop fs -rm hdfs://localhost:9000/user/hduser/input/people.txt

##SBT###
sbt package
sbt run


##create RDD##
val data = 1 to 100
val distData = sc.parallelize(data)
distData.filter(_<10).collect
distData.filter(_<10).foreach(println)






##Size of all the lines
val lines = sc.textFile("data.txt")
val lineLengths = lines.map(s => s.length)
val totalLength = lineLengths.reduce((a, b) => a + b)


##word count
val textFile = sc.textFile("hdfs://localhost:9000/user/hduser/input/textinput.txt")
val count = textFile.flatMap( x => x.split(" ")).map(x => (x,1)).reduceByKey(_+_)
val count = textFile.flatMap( x => x.split(" ")).map(x => (x,1)).reduceByKey((a,b) => (a+b))
textFile.map( x => x.split(" ").size).reduce((a,b) => Math.max(a,b))

field = "hello"
textFile.map( x => x + field).flatMap( x => x.split(" ")).map( x => x


val distData = distData3.zip(distData1)
distData.sortByKey(false).collect
distData.sortByKey(true).collect



zip
val a = sc. parallelize (1 to 10 , 3)
val b = sc. parallelize (11 to 20 , 3)
val c = sc. parallelize (21 to 30 , 3)
a.zip(b).zip(c).map(x => (x._1._1 , x._1._2 , x._2 )).collect


val a = sc. parallelize (1 to 10 , 3)
a.foreach( x => accum.add(x))
accum.value


sc:
textFile("/my/directory"), textFile("/my/directory/*.txt"), and textFile("/my/directory/*.gz").
SparkContext.wholeTextFiles

val a = sc. parallelize ( List (1, 2, 1, 3) , 1)
val a = sc.parallelize( 1 to 100, 4)

partitions
val c = sc. parallelize (21 to 30 , 3).partitions
c: Array[org.apache.spark.Partition] = Array(org.apache.spark.rdd.ParallelCollectionPartition@691, org.apache.spark.rdd.ParallelCollectionPartition@692, org.apache.spark.rdd.ParallelCollectionPartition@693)

val a = sc.parallelize (21 to 300 , 6)
a.partitions.length

val b = a.coalesce(2, true)
b.partitions.length



###DATA STREAM####
case class Person(name: String, age: Long, salary: Long, birthday: String)
#case clascase class Person(name: String, age: Long, salary: Long, birthday: String)s newPerson(name: String, age: Long, salary: Long, birthday: Timestamp)
val peopleDF = sc.textFile("hdfs://localhost:9000/user/hduser/input/people.txt")
val plDF = peopleDF.map(x => x.split(",")).map(attributes=>Person(attributes(0),attributes(1).trim.toInt, attributes(2).trim.toInt, attributes(3))).toDF

plDF.select("name","age").show(2)   plDF.select("name","age").show(2,false)
plDF.select("name").groupBy("name").count().orderBy("count").show
plDF.select("*").orderBy("salary").show

someRDD.toDF("name","age","salary","birthday")




plDF.filter(plDF.col("age").gt(18)).select("age","name").show
plDF.filter(plDF.col("age").between(21,30)).select("age","name").show
plDF.filter(plDF.col("name").startsWith("lin")).show



String Concatenate:
plDF.select(upper(plDF.col("name"))).show

Date Function:
plDF.filter(to_date(col("birthday")) > to_date(lit("1998-12-01"))).show //only YYYY-MM-DD can be used
plDF.select(current_date()).show()

Misc Funstion:
plDFRep.select(unix_timestamp()).show()
plDF.filter(col("name").startsWith("lin")).show
plDF.select(col("age").plus(10)).show
plDF.select(plDF.col("name").substr(1,3)).show
plDF.select(plDF.col("name").rlike("lin")).show
plDF.select(plDF.col("age").minus(2)).show
plDF.filter(plDF.col("age").between(21,30)).select("age","name").show
plDFRep.agg(avg("age"),max("age"),min("age")).show
plDFRep.describe("age","name").show
plDFRep.select(year(current_date())).show()
plDF.select("name","age").show(2)   plDF.select("name","age").show(2,false)
plDF.select("name").groupBy("name").count().orderBy("count").show
plDF.select("*").orderBy("salary").show
plDFRep.filter("age > 19").show
plDF.where(col("age").gt(19)&&to_date(col("birthday")) > to_date(lit("1998-12-01"))).show


Misc Expression:
plDF.select(col("age")+10).show
plDF.where("age > 15").show
plDF.filter("age > 15").show







plDF.createOrReplaceTempView("people")

spark.sql("SELECT * FROM people").show(2)


plDF.select(plDF.col("name"), plDF.col("age"))
plDF.select(plDF.col("name"), plDF.col("age"), plDF.col("salary"), plDF.col("birthday")).show

plDF.write.parquet("hdfs://localhost:9000/user/hduser/input/people.parquet")
plDF.write.json("hdfs://localhost:9000/user/hduser/input/people.parquet")
plDF.write.csv("hdfs://localhost:9000/user/hduser/input/people.parquet")

plDF.schema
plDF.printSchema

fileSchema = StructType(StructField('Name',String




// To create Dataset[Row] using SparkSession
val people = spark.read.parquet("...")
val department = spark.read.parquet("...")

people.filter("age > 30")
  .join(department, people("deptId") === department("id"))
  .groupBy(department("name"), "gender")
  .agg(avg(people("salary")), max(people("age")))








df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'],'D': ['D0', 'D1', 'D2', 'D3']},index=[0, 1, 2,3])
