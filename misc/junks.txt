www.greycampus.com
 
username -yuhaitao@yahoo.com
 


elearning@greycampus.com

https://www.youtube.com/watch?v=bCVZpBd5C9g

cd $HADOOP_HOME
bin/hadoop
bin/start-all.sh
jps
open browser
localhost_50070

bin/stop-all.sh

student/1234567890


/home/student/Installations/hadoop-1.2.1/bin

hdfs getconf -confKey fs.defaultFS


<property>
<name>dfs.replication</name>
<value>2</value>
<description>Block Replication Factor</description>
</property>

hive-0.12.0.tar.gz



show databases;
load data local inpath '/home/student/Desktop/EmployeeDetails.txt' into table employee;
create table employee(emp_id int, name string, salary double) row format delimited fields terminated by ',';
create database wordcount;

syntax:
create table sample (name sting comment "Name of the employee", id int, mobileNo bigint, salary float) row format delimited fields terminated by ',' lines terminated by '\n';

set hive.cli.print.current.db=true;

ifconfig
impala-shell 

load data inpath '/Sample/Batting.csv' into table temp_batting;


>SELECT
	>regexp_extract(col_value, '^(?:([^,]*)\,?){1}', 1) player_id,
	>regexp_extract(col_value, '^(?:([^,]*)\,?){2}', 1) year,
	>regexp_extract(col_value, '^(?:([^,]*)\,?){9}', 1) run
	>from temp_batting;



export HIVE_CONF_DIR=$HIVE_CONF_DIR
export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH
export HADOOP_HOME=

core-site.xml
mapred-site.xml
hdfs-site.xml

http://hortonwork.com/

CREATE TABLE order_items (
)
PARTIONED BY (order_month string)
STORED AS TEXTFILE;


dfs -ls /user/hive/warehouse;


cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ExternalTables
https://github.com/


https://cwiki.apache.org/confluence/display/Hive/LanguageManual

dfs -cat 

http://pig.apache.org/




http://archive.apache.org/dist/hadoop/pig/

example.pig
bagA= Load '/employee.txt' using PigStorage ('\t') as (FirstName:chararry; LastName:chararray, MobileNumber:chararray, City:chararray, Profession:chararray);
bagB = FOREACH bagA generate FirstName, MobileName;
STORE bagB into 'exampleoutput';
dump bagB


example2
bagA=Load '/Sampletextfile.txt' USING PigStorage() AS (event:chararray, user:chararray);
bagB=foreach bagA generate flatten(TOKENIZE((chararray)$0)) as word;
bagC=group bagB by word;
bagD=foreach bagC generate count(bagB), group;
dump bagD
c = UNION a,b;
SPLIT c INTO d IF $0 == 0, e IF $0 == 1;

f = FILTER c BY $1 > 3;

illustrate f;

k = FOREACH c GENERATE a2, a2*a3

FOREACH a GENERATE *;

word count= file
haddop edureka java day night learn
haddop edureka java day night learn
haddop edureka java day night learn
haddop edureka java day night learn

hadoop

C JOIN A BY $0, B By $0

hadoop pig group
pig aggregate functions

hadoopexamp.com




bag and tuple

cloudera quick start 5.8

hthortonworks sandbox 2.5>>http://hortonworks.com/downloads/#sandbox


Wen Ye:
hthortonworks sandbox 2.5

git clone https://github.com/yuhaitao10/hadoop-admin-utils.git


/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
    val conf = new SparkConf().setAppName("Simple Application")
    val sc = new SparkContext(conf)
    val logData = sc.textFile(logFile, 2).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
  }
}

hdfs://localhost:9000/user/hduser/input/textinput.txt
sudo apt-get install git
sudo apt-get install yum
yum install git
hadoop version


http://archive.apache.org/dist/hadoop/pig/

download and certification website
https://databricks.com/spark/certification

http://www.cloudera.com/downloads/quickstart_vms/5-8.html>>>https://www.cloudera.com/user/login.html

github.com


http://www.javachain.com/pig-aggregate-functions

https://github.com/yuhaitao10/hadoop-admin-utils



http://127.0.0.1:8080/#/login

hortonworks.com

purchase a PC
https://www.amazon.com/Dell-Inspiron-Touchscreen-Quad-Core-Processor/dp/B01GQG9E2G/ref=sr_1_5?s=electronics&ie=UTF8&qid=1476646431&sr=1-

Creating an RDD
val wordsRDD = sc.parallelize((Array("Nobody", "expects", "the", "Spahish", Inquisition"));

val linesRDD = sc.textFile("s3n://my-spark-bucket/foobar.txt")
5&keywords=laptop+16gb+ram


SPARK training:
https://spark-summit.org/2014/training/

gist.github.com/ceteri/f2c3486062c9610eac1d
gist.github.com/ceteri/8ae5b9509a08c08a1132
gist.github.com/ceteri/11381941

Driver Action: collect(), count()--->>>bottle neck for I/O

Distributed Action: saveAsTextFile(), foreach()


SPARK Tutorial:
https://www.youtube.com/watch?v=qgkv-UapcVE
https://www.youtube.com/watch?v=2STfulBcorA
spark tutorial 

http://spark.apache.org/docs/latest/sql-programming-guide.html

https://www.youtube.com/watch?v=7ooZ4S7Ay6Y

SPARK 2.0



https://www.youtube.com/watch?v=K14plpZgy_c done


STREAM:
import org.apache.spark.streaming._
val ssc = StreaminContext(sc, Seconds(1)))
//Connecto to the stream on port 7777 of the streaming host.
val linesStream = ssc.socketTextStream("somehost", 7777)

//Filter out DStream for line with "error" and dump the 
//matching line to a file.
lineStream.filter {_contains "errot"}
          .foreachRDD{_.saveAsTextFile("...") }

// Start the streaming context and wait for it to "finish"
ssc.start()
ssc.awaitTermination()


##
val errorDF = df.filter($"msgType" === "ERROR");
errorDF.registerTempTable("log");

%sql SELECT timestamp, message FROM logs WHERE message LIKE "Device%"
val x = sqlContext("SELECT timestamp, message FROM logs WHERE message LIKE 'Device%'"
s.show
